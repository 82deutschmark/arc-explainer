# Worm Arena: Local Game Analysis Insights (December 2025)

## Overview
This report summarizes insights gathered from analyzing 1,472 local game replays stored in the `external/SnakeBench` submodule. The analysis focuses on performance benchmarks, cost efficiency, and edge cases identified during the month of December 2025.

## Performance Benchmarks (The "Pro" Tier)
We define "Pro Matches" as games where at least one player scored **25 or more apples**.

*   **Current Local Record**: **30 apples** (104 rounds)
    *   **Game ID**: `5edcbc7b...`
    *   **Matchup**: `openai/gpt-5.1-codex-mini` (Winner) vs `nvidia/nemotron-3-nano-30b-a3b:free`.
    *   **Insight**: GPT-5.1 Codex Mini demonstrates significantly higher survival and efficiency than its peers, often winning even when the opponent scores highly (30 vs 21 in this case).

*   **Runner Up**: **29 apples** (101 rounds)
    *   **Game ID**: `8f7da248...`
    *   **Matchup**: `openai/gpt-5.1-codex-mini` (Winner) vs `openai/gpt-5.2`.

## The "Hall of Shame" (Worst Matches)
A significant number of matches end in **Round 1 with 0 apples**.

*   **Common Culprits**: Smaller open-source models like `Llama 3.2 11V`, `Qwen2.5 Coder 7B`, and `Phi 4` frequently fail immediately.
*   **Death Reasons**: Most "Quick Deaths" are caused by `body_collision` (the snake immediately turns back into itself) or `off_board` moves on the first turn.
*   **Insight**: This suggests that without "Chain of Thought" or higher reasoning effort, many smaller models struggle with the coordinate system or immediate spatial awareness.

## Cost & Efficiency Analysis
*   **Most Expensive Match**: **$3.0660** (35 rounds)
    *   **Matchup**: `google/gemini-2.5-flash-preview-09-2025` vs `google/gemini-3-pro-preview`.
    *   **Insight**: High-reasoning models with large internal "thoughts" or rationales significantly drive up costs. The frontend should consider a "Cost per Apple" or "Cost per Turn" metric to identify the most efficient thinkers.

## Time & Duration Insights
*   **Longest Survival**: **141 rounds** (ID: `bc8bb584...`) - `deepseek/deepseek-v3.2` vs `deepseek/deepseek-chat-v3.1`.
*   **Longest Thinking Time**: One match (`d24ac1c2...`) lasted **3.7 hours** for only 63 rounds.
    *   **Insight**: This indicates severe latency or extremely large rationales being generated by the models involved.

---

## Recommendations for Frontend Improvements

The current `WormArenaDistributions.tsx` and `WormArenaStats.tsx` can be enhanced using logic from the `analyze_local_games.py` script:

1.  **"Greatest Hits" & "Hall of Shame" Callouts**:
    *   Implement small summary cards for the current local record holder (Max Apples) and the "Quickest Death."
    *   This provides immediate context for what "good" and "bad" performance looks like.

2.  **Winner Detection Robustness**:
    *   The frontend should mirror the script's improved winner detection, which checks both `game.winner_id` and the individual `player.result === "won"` fields to handle edge cases where the game-level ID might be missing.

3.  **Cost Efficiency Visualization**:
    *   Add a chart for **"Cost vs. Score"**. Identify "Budget Champions" (models that score high at low cost) vs. "Expensive Failures."

4.  **Date Range Filtering**:
    *   Allow users to filter distributions by "Last 30 Days" or specific months. The current distribution page shows everything at once, which can obscure recent performance improvements in models.

5.  **Rational Truncation Context**:
    *   Since expensive matches are often driven by rationale length, the frontend could show a "Total Tokens Generated" stat per model to help us identify which ones are being too verbose.
